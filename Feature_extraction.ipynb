{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLXzD2jsoetsMzmYWWBwzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lindyco/Colab/blob/main/Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrdmy0FjNKBA"
      },
      "source": [
        "This program is to extract the features from the website and use classification to predict the page number of contents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ORd-2dyUNGn"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import urllib.request as req\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtPzgTfla8bl"
      },
      "source": [
        "# Import the data from the web page\n",
        "\n",
        "url1='http://quotes.toscrape.com/page/1/'\n",
        "url2='http://quotes.toscrape.com/page/2/'\n",
        "\n",
        "sourcedata1 = req.urlopen(url1)\n",
        "soup1=bs(sourcedata1,\"html.parser\")\n",
        "\n",
        "sourcedata2 = req.urlopen(url2)\n",
        "soup2=bs(sourcedata2,\"html.parser\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtpgouT9a-90",
        "outputId": "ee68bb32-e5e6-4636-8be9-fc548925cf7d"
      },
      "source": [
        "# Get all the quotes from each page\n",
        "\n",
        "quotes = soup1.find_all(\"span\",{\"class\":\"text\"}) + soup2.find_all(\"span\",{\"class\":\"text\"}) \n",
        "length = len(quotes)\n",
        "length"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX64p654bCc5",
        "outputId": "cfb58022-827c-4f5d-900d-02f5790833f5"
      },
      "source": [
        "content1 = []\n",
        "for each in quotes:\n",
        "    txt = each.text.strip()\n",
        "    content1.append(txt)\n",
        "    \n",
        "content1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
              " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
              " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
              " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
              " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
              " '“Try not to become a man of success. Rather become a man of value.”',\n",
              " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
              " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
              " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
              " '“A day without sunshine is like, you know, night.”',\n",
              " \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\",\n",
              " '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”',\n",
              " \"“If you can't explain it to a six year old, you don't understand it yourself.”\",\n",
              " \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\",\n",
              " '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”',\n",
              " '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”',\n",
              " \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\",\n",
              " '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”',\n",
              " '“Good friends, good books, and a sleepy conscience: this is the ideal life.”',\n",
              " '“Life is what happens to us while we are making other plans.”']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbJHxOprbFAW",
        "outputId": "2164fe95-0486-492a-8027-76f24f1457d1"
      },
      "source": [
        "# Get all the authors from each page\n",
        "\n",
        "authors = soup1.find_all(\"small\",{\"class\": \"author\"}) + soup2.find_all(\"small\",{\"class\": \"author\"})\n",
        "content2 = []\n",
        "for each in authors:\n",
        "    txt = each.text.strip()\n",
        "    content2.append(txt)\n",
        "content2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Albert Einstein',\n",
              " 'J.K. Rowling',\n",
              " 'Albert Einstein',\n",
              " 'Jane Austen',\n",
              " 'Marilyn Monroe',\n",
              " 'Albert Einstein',\n",
              " 'André Gide',\n",
              " 'Thomas A. Edison',\n",
              " 'Eleanor Roosevelt',\n",
              " 'Steve Martin',\n",
              " 'Marilyn Monroe',\n",
              " 'J.K. Rowling',\n",
              " 'Albert Einstein',\n",
              " 'Bob Marley',\n",
              " 'Dr. Seuss',\n",
              " 'Douglas Adams',\n",
              " 'Elie Wiesel',\n",
              " 'Friedrich Nietzsche',\n",
              " 'Mark Twain',\n",
              " 'Allen Saunders']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbOchglXHPPk",
        "outputId": "ce47df23-9f31-4206-9f87-262146f972ab"
      },
      "source": [
        "#Assign the page to base on the index\n",
        "\n",
        "content3 = []\n",
        "for num in range(length):\n",
        "  if(0 <= num <= 10):\n",
        "    content3.append(\"page0\")\n",
        "  if(10 < num <= 20):\n",
        "    content3.append(\"page1\")\n",
        "content3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page0',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1',\n",
              " 'page1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnqlo_vEbHqW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "e96de8fc-c0be-4620-81fa-5598c4dcaaa8"
      },
      "source": [
        "#Build up data frame to store the collected data\n",
        "df = pd.DataFrame(data = {'quote':content1, 'author':content2, 'page':content3})\n",
        "\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quote</th>\n",
              "      <th>author</th>\n",
              "      <th>page</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“The world as we have created it is a process ...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“It is our choices, Harry, that show what we t...</td>\n",
              "      <td>J.K. Rowling</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>“There are only two ways to live your life. On...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“The person, be it gentleman or lady, who has ...</td>\n",
              "      <td>Jane Austen</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“Imperfection is beauty, madness is genius and...</td>\n",
              "      <td>Marilyn Monroe</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>“Try not to become a man of success. Rather be...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>“It is better to be hated for what you are tha...</td>\n",
              "      <td>André Gide</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>“I have not failed. I've just found 10,000 way...</td>\n",
              "      <td>Thomas A. Edison</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>“A woman is like a tea bag; you never know how...</td>\n",
              "      <td>Eleanor Roosevelt</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>“A day without sunshine is like, you know, nig...</td>\n",
              "      <td>Steve Martin</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>“This life is what you make it. No matter what...</td>\n",
              "      <td>Marilyn Monroe</td>\n",
              "      <td>page0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>“It takes a great deal of bravery to stand up ...</td>\n",
              "      <td>J.K. Rowling</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>“If you can't explain it to a six year old, yo...</td>\n",
              "      <td>Albert Einstein</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>“You may not be her first, her last, or her on...</td>\n",
              "      <td>Bob Marley</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>“I like nonsense, it wakes up the brain cells....</td>\n",
              "      <td>Dr. Seuss</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>“I may not have gone where I intended to go, b...</td>\n",
              "      <td>Douglas Adams</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>“The opposite of love is not hate, it's indiff...</td>\n",
              "      <td>Elie Wiesel</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>“It is not a lack of love, but a lack of frien...</td>\n",
              "      <td>Friedrich Nietzsche</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>“Good friends, good books, and a sleepy consci...</td>\n",
              "      <td>Mark Twain</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>“Life is what happens to us while we are makin...</td>\n",
              "      <td>Allen Saunders</td>\n",
              "      <td>page1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                quote  ...   page\n",
              "0   “The world as we have created it is a process ...  ...  page0\n",
              "1   “It is our choices, Harry, that show what we t...  ...  page0\n",
              "2   “There are only two ways to live your life. On...  ...  page0\n",
              "3   “The person, be it gentleman or lady, who has ...  ...  page0\n",
              "4   “Imperfection is beauty, madness is genius and...  ...  page0\n",
              "5   “Try not to become a man of success. Rather be...  ...  page0\n",
              "6   “It is better to be hated for what you are tha...  ...  page0\n",
              "7   “I have not failed. I've just found 10,000 way...  ...  page0\n",
              "8   “A woman is like a tea bag; you never know how...  ...  page0\n",
              "9   “A day without sunshine is like, you know, nig...  ...  page0\n",
              "10  “This life is what you make it. No matter what...  ...  page0\n",
              "11  “It takes a great deal of bravery to stand up ...  ...  page1\n",
              "12  “If you can't explain it to a six year old, yo...  ...  page1\n",
              "13  “You may not be her first, her last, or her on...  ...  page1\n",
              "14  “I like nonsense, it wakes up the brain cells....  ...  page1\n",
              "15  “I may not have gone where I intended to go, b...  ...  page1\n",
              "16  “The opposite of love is not hate, it's indiff...  ...  page1\n",
              "17  “It is not a lack of love, but a lack of frien...  ...  page1\n",
              "18  “Good friends, good books, and a sleepy consci...  ...  page1\n",
              "19  “Life is what happens to us while we are makin...  ...  page1\n",
              "\n",
              "[20 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwfOzJZTz6Zi"
      },
      "source": [
        "# This is the text feature I extract from the website and convert to csv file\n",
        "df.to_csv(\"quotesCollection.csv\",encoding = 'utf-8-sig')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rrc9VDN1k7B"
      },
      "source": [
        "After create the csv file, I going to training and predict the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gF8B6taz6Mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a285b8-d8a2-4ab6-86c7-1216a5ae3120"
      },
      "source": [
        "# Import and download necessary library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUuGKOeXz5_9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbUErxTgz5Jb"
      },
      "source": [
        "#Set Random seed to 500 to generate a random number\n",
        "np.random.seed(500)\n",
        "\n",
        "# Import the Data that I scraped using pandas\n",
        "Quotes = pd.read_csv(r\"quotesCollection.csv\",encoding='latin-1')\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ul0ygdb8iaF"
      },
      "source": [
        "# Data Pre-processing - This will help in getting better results through the classification algorithms\n",
        "\n",
        "# Remove blank rows if any.\n",
        "Quotes['quote'].dropna(inplace=True)\n",
        "\n",
        "# Change all the text to lower case. \n",
        "Quotes['quote'] = [entry.lower() for entry in Quotes['quote']]\n",
        "\n",
        "# Tokenization : In this each entry in the file quotes will be broken into set of words\n",
        "Quotes['quote']= [word_tokenize(entry) for entry in Quotes['quote']]\n",
        "\n",
        "# Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
        "\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrXyL7KZ5D65",
        "outputId": "6c6182bc-47ca-4305-ba73-bbf17641f35b"
      },
      "source": [
        "for index,entry in enumerate(Quotes['quote']):\n",
        "    # Declaring a empty list to store the words\n",
        "    Final_words = []\n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(entry):\n",
        "        # Below condition is to check if is alphabets and not the stop words\n",
        "        if word.isalpha() and word not in stopwords.words('english') :\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    # The final processed set of words for each iteration will be stored in 'text_final' as string\n",
        "    Quotes.loc[index,'text_final_str'] = str(Final_words)\n",
        "\n",
        "print(Quotes['text_final_str'].head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    ['world', 'create', 'process', 'thinking', 'ch...\n",
            "1          ['choice', 'harry', 'show', 'truly', 'far']\n",
            "2    ['two', 'way', 'live', 'life', 'one', 'though'...\n",
            "3    ['person', 'gentleman', 'lady', 'pleasure', 'g...\n",
            "4    ['beauty', 'madness', 'genius', 'good', 'absol...\n",
            "Name: text_final_str, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBlaxDu78agw"
      },
      "source": [
        "# Split the model into Train(70%) and Test Data(30%) set\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Quotes['text_final_str'],Quotes['page'],test_size=0.2, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oooiWJit840U"
      },
      "source": [
        "# Page number encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRw19UJU9Pzj"
      },
      "source": [
        "# Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the Quote\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(Quotes['text_final_str'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb7JdnYpA_qN",
        "outputId": "7bad2298-fc65-4701-baf6-ab28da218201"
      },
      "source": [
        "# Now we can run different algorithms to classify out data check for accuracy\n",
        "\n",
        "# Classifier - Algorithm - Naive Bayes\n",
        "# fit the training dataset on the classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "print(predictions_NB)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0]\n",
            "Naive Bayes Accuracy Score ->  50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7EvMVHcBNw4",
        "outputId": "d010b4a4-e2ee-4b71-f149-1c4ebd2af17f"
      },
      "source": [
        "# Classifier - Algorithm - SVM\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "print(predictions_SVM)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0]\n",
            "SVM Accuracy Score ->  50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1XgqZg6F7RV"
      },
      "source": [
        "------------------------------------------------------------------------------------------------\n",
        "\n",
        "Accuracy of both classification model are the same 50%, I think the reason of that is this program predict the page number, it is not a categories that relative to any words, but the classification model Naive Bayes and SVM is predict things by the words using statistics.\n",
        "Therefore, I decide to make another csv file base on the categorie of the quetoes, such as categorie life and humor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gZA3Y4zDDlP"
      },
      "source": [
        "# Create the csv file quotetype\n",
        "\n",
        "url1='http://quotes.toscrape.com/tag/life/'\n",
        "url2='http://quotes.toscrape.com/tag/humor/'\n",
        "\n",
        "sourcedata1 = req.urlopen(url1)\n",
        "soup1=bs(sourcedata1,\"html.parser\")\n",
        "\n",
        "sourcedata2 = req.urlopen(url2)\n",
        "soup2=bs(sourcedata2,\"html.parser\")\n",
        "\n",
        "quotes = soup1.find_all(\"span\",{\"class\":\"text\"}) + soup2.find_all(\"span\",{\"class\":\"text\"}) \n",
        "length = len(quotes)\n",
        "length\n",
        "\n",
        "content1 = []\n",
        "for each in quotes:\n",
        "    txt = each.text.strip()\n",
        "    content1.append(txt)\n",
        "    \n",
        "content1\n",
        "\n",
        "authors = soup1.find_all(\"small\",{\"class\": \"author\"}) + soup2.find_all(\"small\",{\"class\": \"author\"})\n",
        "content2 = []\n",
        "for each in authors:\n",
        "    txt = each.text.strip()\n",
        "    content2.append(txt)\n",
        "content2\n",
        "\n",
        "# type_0 is type life and type_1 is humor\n",
        "content3 = []\n",
        "for num in range(length):\n",
        "  if(0 <= num <= 10):\n",
        "    content3.append(\"type_0\")\n",
        "  if(10 < num <= 20):\n",
        "    content3.append(\"type_1\")\n",
        "content3\n",
        "\n",
        "df = pd.DataFrame(data = {'quote':content1, 'author':content2, 'type':content3})\n",
        "\n",
        "df.to_csv(\"quotetype.csv\",encoding = 'utf-8-sig')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsGWoEIBDxq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783a5c09-3d25-448a-ebef-ac6385c3f3a7"
      },
      "source": [
        "# Data prepare for the prediction models\n",
        "\n",
        "Quotes_type = pd.read_csv(r\"quotetype.csv\",encoding='latin-1')\n",
        "\n",
        "Quotes_type['quote'].dropna(inplace=True)\n",
        "\n",
        "Quotes_type['quote'] = [entry.lower() for entry in Quotes_type['quote']]\n",
        "\n",
        "Quotes_type['quote']= [word_tokenize(entry) for entry in Quotes_type['quote']]\n",
        "\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "for index,entry in enumerate(Quotes_type['quote']):\n",
        "    Final_words = []\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "\n",
        "    for word, tag in pos_tag(entry):\n",
        "        if word.isalpha() and word not in stopwords.words('english') :\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    Quotes_type.loc[index,'text_final_str'] = str(Final_words)\n",
        "\n",
        "print(Quotes_type['text_final_str'].head())\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    ['two', 'way', 'live', 'life', 'one', 'though'...\n",
            "1                              ['well', 'hat', 'love']\n",
            "2    ['life', 'make', 'matter', 'go', 'mess', 'some...\n",
            "3    ['may', 'go', 'intend', 'go', 'think', 'end', ...\n",
            "4    ['friend', 'good', 'book', 'sleepy', 'conscien...\n",
            "Name: text_final_str, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6lodD2tJIU3"
      },
      "source": [
        "# Set up the training and testing data sets\n",
        "\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Quotes_type['text_final_str'],Quotes_type['type'],test_size=0.2, random_state=42)\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(Quotes_type['text_final_str'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHY2bVAOJLBP",
        "outputId": "cfd8f595-bf88-4a9e-ba9a-dafcfbf2b579"
      },
      "source": [
        "# Native bayes model\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "print(predictions_NB)\n",
        "\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "\n",
        "# SVM model\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "print(predictions_SVM)\n",
        "\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0]\n",
            "Naive Bayes Accuracy Score ->  75.0\n",
            "[0 0 0 0]\n",
            "SVM Accuracy Score ->  50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn5XuqcBK0Ge"
      },
      "source": [
        "This time, two models return different accuracy, Naive Bayes has higher accuracy score than SVM model."
      ]
    }
  ]
}